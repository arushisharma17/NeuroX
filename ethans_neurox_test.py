# -*- coding: utf-8 -*-
"""Ethans_NeuroX_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15IMIJhScKd6D1nqvHyUWw641dOrvB8P2

## Tutorial Notebook to use Neurox (modified library)

After adding your functionality to the library, make sure to add a usage example here to demonstrate how it works so everyone else can quickly get upto speed.

Link to repository: https://github.com/arushisharma17/NeuroX.git

Link to (original) NeuroX documentation: https://neurox.qcri.org/docs/

BERTConceptNet:
https://neurox.qcri.org/projects/bert-concept-net.html

Original ConceptX repository: https://github.com/hsajjad/ConceptX

### Installation and imports
"""
import subprocess

"""### TODO: RAID Tool
Use RAID tool to generate datasets and use in place of below .in and .label files

### Input files: test.in and test.label

We use sample files with 10 lines of tokenized Java code and corresponding labels. They are obtained from code snippets from the CodeSyntax dataset and the labels were generated using Javalang. We will be using tree-sitter instead of javalang moving forward.
"""

# os.system("touch /NeuroX/examples/test.in")

# Commented out IPython magic to ensure Python compatibility.
# os.system("cd /content/NeuroX/examples/")
# os.system("touch /NeuroX/examples/test.label")

"""### Extract representations(features/neuron activations) for specific layer

The above code is used to extract activations for all layers. In the following code, we extract the activations/representations/features for specific layers by setting decompose layers and filter_layers

#### Structure of Each Line in the JSON File:

- **linex_idx**: Sentence index
- **features**: List of tokens (with their activations)
  - **token**: The current token
  - **layers**: List of layers
    - **index**: Layer index (does not correspond to original model’s layers)
    - **values**: List of activation values for all neurons in the layer


TODO: add functionality to supply multiple layers adn get corresponding files.
"""


layer=1 #Can specify comma separated list of layers to get activations for
#model='bert-base-uncased'
model = 'microsoft/codebert-base'
working_file = '/content/NeuroX/examples/test.in'
download_bert = False

if download_bert:
    import NeuroX.neurox.data.extraction.transformers_extractor as transformers_extractor
    transformers_extractor.extract_representations(model,
        'NeuroX/examples/test.in',
        'NeuroX/examples/test_layer_activations.json', #modify for multiple layers
        aggregation="average", #last, first
        output_type="json",
        decompose_layers=True,  # Enable layer-wise decomposition
        filter_layers=str(layer),
    )


"""### Process/filter activations to use with clustering algorithms

'''
This script loads token activations and corresponding token data from a transformer model,
prepares a dataset with token representations, activation vectors, and labels, and saves the dataset
in JSON format. The processed data includes sentences, labels, and token activations, which can be used
for further analysis or model training tasks.

Outputs:
- <output_prefix>-sentences.json: List of tokenized sentences.
- <output_prefix>-labels.json: Labels associated with each sentence.
- <output_prefix>-dataset.json: Token representations and their corresponding activation vectors.
'''
"""

from NeuroX.NeuroXCode.src.process_activations.activations_processor import DatasetProcessor

# Step 1: Define full paths to the input files
activations_file = 'NeuroX/examples/test_layer_activations-layer1.json'
tokens_file = 'NeuroX/examples/test.in'
labels_file = 'NeuroX/examples/test.label'
dataset_processor = DatasetProcessor()  # Initialize the dataset processor
output_prefix = 'NeuroX/examples/test'  # Use the directory and prefix based on the tokens file


# Step 2: Load activations and tokens using the full paths
activations, tokens = dataset_processor.load_activations_and_tokens(activations_file, tokens_file, labels_file)

# Step 3: Prepare dataset and save using output_prefix
token_dataset = dataset_processor.prepare_dataset(activations, tokens, output_prefix)
#
# """#### frequency_count.py: The script will output the following:
#
# 1. A JSON file containing the word counts.
# 2. Printed statistics in the console, including:
#    - List of singleton words (words that appear only once).
#    - Total number of unique words (types).
#    - Total number of words (tokens).
#    - Number of words occurring less than 2, 3, 4, and 5 times.
#
# ### Output Format
#
# The output JSON file will contain a dictionary where:
#
# - **Keys**: Words from the input files.
# - **Values**: The count of occurrences for each word.
#
# Example JSON output:
#
# \```json
# {
#   "word1": 5,
#   "word2": 2,
#   "word3": 1
# }
# \```
# """
# Step 4: Process dataset with frequency filtering and generate vocab and points
from NeuroX.NeuroXCode.src.process_activations.frequency_wordcount import word_count_from_files

input_files = ['NeuroX/examples/test_token_activations.json']
word_count_file = f"{output_prefix}_output_activtations"
word_count = word_count_from_files(input_files)

#frequency_filter.py
# Frequency thresholds
from NeuroX.NeuroXCode.src.process_activations.frequency_filtering import load_sentences, filter_dataset, load_word_count, save_to_file, print_statistics
min_freq = 5
max_freq = 50
del_freq = 500000
sentence_file = 'NeuroX/examples/test_token_sentences.json'

print(f"Min: {min_freq} Max: {max_freq} Del: {del_freq}")

# word_count = load_word_count(word_count_file)
sentences, files_size = load_sentences(sentence_file)

filtered_output, temp = filter_dataset(input_files[0], word_count, files_size,
                                       min_freq, max_freq, del_freq, False)
(maxskip, minskip, delskip, maxskips, minskips, delskips) = temp

# Save output data to files (you could skip this if you just want returned data)
save_to_file(sentences, f"{output_prefix}_output__min_{min_freq}_max_{max_freq}-sentences.json")
save_to_file(filtered_output, f"{output_prefix}_output_activtations_min_{min_freq}_max_{max_freq}_del_{del_freq}-dataset.json")

# Print statistics
print_statistics(word_count, maxskip, minskip, delskip, maxskips, minskips, delskips)

# Run the process
from NeuroX.NeuroXCode.src.process_activations.vocab_points import generate_vocab_and_points

generate_vocab_and_points(filtered_output, output_prefix)

"""Got processed-activations-dataset.json, processed-activations-sentences.json files

## Data Structure Overview (Points and vocab files)
"""

import numpy as np

# Load the .npy file
points_file = 'NeuroX/examples/test_processed_points.npy'
vocab_file = 'NeuroX/examples/test_processed_vocab.npy'
points_file_loaded = np.load('NeuroX/examples/test_processed_points.npy')
vocab_file_loaded = np.load('NeuroX/examples/test_processed_vocab.npy')
# Print the contents of the numpy array
# If it's a large array, you can also view its shape and data type
print("Points Shape:", points_file_loaded.shape)
print("Points Data type:", points_file_loaded.dtype)
print("Vocab Shape:", vocab_file_loaded.shape)
print("Vocab Data type:", vocab_file_loaded.dtype)


### DID NOT GO FURTHER THAN THIS ###
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from collections import defaultdict
import time
import argparse
import dill as pickle
from memory_profiler import profile


print("USAGE: create_agglomerative_clustering.py -p <POINT_FILE> -v <VOCAB_FILE> -k <CLUSTERS> -o <OUTPUT_FOLDER>")


@profile
def agglomerative_cluster(points, vocab, K, output_path, ref=''):
    """
    Uses the point.npy, vocab.npy files of a layer (generated using https://github.com/hsajjad/ConceptX/ library) to produce a clustering of <K> clusters at <output_path> named clusters-agg-{K}.txt
    """
    print('Starting agglomerative clustering...')
    clustering = AgglomerativeClustering(n_clusters=K,compute_distances=True).fit(points)
    print('Finished clustering')
    fn = f"{output_path}/model-{K}-agglomerative-clustering{ref}.pkl"
    with open(fn, "wb") as fp:
        pickle.dump(clustering,fp)

    clusters = defaultdict(list)
    for i,label in enumerate(clustering.labels_):
        clusters[clustering.labels_[i]].append(vocab[i])

    # Write Clusters in the format (Word|||WordID|||SentID|||TokenID|||ClusterID)
    out = ""
    for key in clusters.keys():
        for word in clusters[key]:
            out += word+"|||"+str(key)+"\n"

    with open(f"{output_path}/clusters-agg-{K}{ref}.txt",'w') as f:
        f.write(out)

    return out

def cluster_test(vocab_file, point_file, output_path, K, point_count_ratio):
    # K is num of clusters

    vocab = np.load(vocab_file)
    original_count = len(vocab)
    useable_count = int(point_count_ratio*original_count) if point_count_ratio != -1 else -1
    vocab = np.load(vocab_file)[:useable_count]
    points = np.load(point_file)[:useable_count, :]

    start_time = time.time()
    ref = '-' + str(point_count_ratio) if point_count_ratio > 0 else ''
    output = agglomerative_cluster(points, vocab, K, output_path, ref)
    end_time = time.time()

    print(f"Runtime: {end_time - start_time}")
#

if __name__ == "__main__":
    pass
    # cluster_test(vocab_file, points_file, "NeuroX/examples/clustering_output/", 3, .5)

# !python create_agglomerative_clustering.py -p /content/NeuroX/examples/test_processed_points.npy -v /content/NeuroX/examples/test_processed_vocab.npy -k 4 -o /content/NeuroX/examples/clustering_output/
#
# !pip install dill

#!python NeuroX/conceptx/agglomerative.py

from NeuroX.NeuroXCode.src.algorithms.clustering.agglomerative import AgglomerativeClusteringPipeline
# Initialize the pipeline with default parameters
pipeline = AgglomerativeClusteringPipeline(output_path='NeuroX/examples/clustering_output') #num_clusters=4)
# For loading real data
points, vocab = pipeline.load_and_prepare_data(point_file='NeuroX/examples/test_processed_points.npy', vocab_file='/content/NeuroX/examples/test_processed_vocab.npy')

# Run the clustering pipeline
clustering, clusters = pipeline.run_pipeline(points, vocab)

# Plot the dendrogram for visualization
linkage_matrix = pipeline.create_linkage_matrix(points)
pipeline.plot_dendrogram(linkage_matrix,'dendrogram.png')

# !git push

"""##TODO:
1. Add labelling into module https://github.com/arushisharma17/CodeConceptNet/blob/branch_vedant/annotation/500-clusters-output/gemini_labelling.py   (Done)

2. Add alignment/evaluation into module and tutorial
"""

import os
import json

def process_cluster_file(file_path, numbered_sentences, clusters_data):
    """
    Processes a single cluster file, extracting unique tokens and all their associated sentences per cluster.

    Args:
      file_path: The path to the .txt file.
      numbered_sentences: A dictionary mapping sentence IDs to sentences.
      clusters_data: A dictionary to hold data for each cluster.

    Returns:
      Updates clusters_data with unique tokens and their sentences grouped by cluster_id.
    """
    with open(file_path, 'r') as f:
        for line in f:
            # Split line into components: token|||sentence_id|||...|||cluster_id
            parts = line.strip().split('|||')
            token = parts[0]
            sentence_id = parts[2]  # The second part is the sentence ID, starting from 0
            cluster_id = parts[-1]  # The last part is the cluster ID

            # Fetch the sentence from the numbered_sentences, converting the sentence_id to integer
            sentence = numbered_sentences.get(str(int(sentence_id)), "")  # Ensure sentence_id is treated as a string key

            # If the cluster is not in the dictionary, initialize it
            if cluster_id not in clusters_data:
                clusters_data[cluster_id] = {"unique_tokens": [], "Context Sentences": []}

            # If the token is not already in the unique tokens list, add it
            if token not in clusters_data[cluster_id]["unique_tokens"]:
                clusters_data[cluster_id]["unique_tokens"].append(token)

            # Add the sentence if it's not already in the list for this cluster
            if sentence and sentence not in clusters_data[cluster_id]["Context Sentences"]:
                clusters_data[cluster_id]["Context Sentences"].append(sentence)

def process_txt_files(directory, numbered_sentences):
    """
    Loops through all .txt files in a directory and processes each to extract unique tokens and sentences grouped by cluster.

    Args:
      directory: The path to the directory to process.
      numbered_sentences: A dictionary mapping sentence IDs to sentences.

    Returns:
      A dictionary where keys are cluster IDs, and values contain unique tokens and associated sentences.
    """
    clusters_data = {}

    for filename in os.listdir(directory):
        if filename.endswith(".txt"):
            filepath = os.path.join(directory, filename)
            print(f"Processing file: {filepath}")

            # Process the cluster file and update clusters_data
            process_cluster_file(filepath, numbered_sentences, clusters_data)

    return clusters_data

# Load the numbered sentences JSON
with open('/content/NeuroX/examples/clustering_output/numbered_sentences.json', 'r') as f:
    numbered_sentences = json.load(f)

# Directory containing the .txt files
directory_to_process = '/content/NeuroX/examples/clustering_output'  # Replace with your directory path

# Process all .txt files in the directory
clusters_data = process_txt_files(directory_to_process, numbered_sentences)

# Write the combined clusters data to a JSON file
output_json_file = '/content/NeuroX/examples/clustering_output/clusters_data.json'
with open(output_json_file, 'w') as json_file:
    json.dump(clusters_data, json_file, indent=4)

print(f"Clusters data saved to {output_json_file}")

#have to make directory changes as needed
'''
from collections import defaultdict
import google.generativeai as genai
import time
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# Load JSON data
file_path = r"/content/NeuroX/examples/clustering_output/clusters_data.json"
with open(file_path, 'r') as file:
    data = json.load(file)

genai.configure(api_key='AIzaSyDJpOOJ5zsHfRLoz-MP4oWMJsdvXTk2A5s')

# Create the model
generation_config = {
    "temperature": 0.2,         # Lower temperature for more deterministic output
    "top_p": 0.4,               # Nucleus sampling to focus on the most likely tokens
    "top_k": 35,                # Restricts choices to the top 20 tokens
    "max_output_tokens": 200,   # Adjust as needed for the length of your response
    "response_mime_type": "application/json",
}


model = genai.GenerativeModel(
  model_name="gemini-1.5-flash",
  generation_config=generation_config,
  system_instruction="You are a Java Software Developer and researcher experienced in analyzing code. Your task is to examine clusters of Java tokens and their context sentences. Each cluster contains tokens that play specific roles within the context of Java code.",
)

# Initialize a dictionary to store the final output
final_output = []

# Iterate over each cluster and call the Gemini API
for index, cluster_id in enumerate(list(data.keys())[:15]):
    # Check if we need to pause for rate limiting
    if index > 0 and index % 15 == 0 :
        print("Pausing for 60 seconds due to rate limit...")
        time.sleep(60)

    # Extract the tokens and context sentences from the data for the current cluster
    tokens_list = data[cluster_id].get("unique_tokens", [])
    context_sentences = data[cluster_id].get("Context Sentences", [])

    # Prepare the content to send to the Gemini API
    user_message = f"""
You are analyzing a cluster of Java tokens and their context sentences. Each cluster has one or more unique tokens. Your task is to identify the role or function these tokens play within the context of the provided sentences. Focus on understanding what the tokens are achieving in the code and their syntactic or semantic significance.

Guidelines for Analysis:
1. Tokens: Review the provided tokens.
2. Concise Label: Choose a descriptive label that accurately describes the function of the tokens in the code. Use specific terminology where applicable (e.g., Buffer Manipulation, Method Invocation, Parameter Handling, String Concatenation).
3. Semantic Tags: Include 3-5 relevant semantic tags that describe the high level functionality being achieved in the context sentences.
4. Description: Provide a concise description (2-3 sentences) explaining the choice of label and the semantic tags.
5. ' ( ' would have label 'Opening Parenthesis' and ')' would have label 'Closing Parenthesis'

Examples from Previous Clusters:
1. Tokens: returnBuffer, concatBuffer
   Context Sentences:
   - returnBuffer.append(minParam);
   - returnBuffer.append(FieldMetaData.Decimal.SQ_CLOSE);
   - StringBuffer concatBuffer = new StringBuffer();
   - concatBuffer.append(toAdd);


   Label: Buffer Manipulation
   Semantic Tags: StringBuilder, StringBuffer, Data Aggregation, String Concatenation
   Description: The tokens represent StringBuilder and StringBuffer objects used for building strings by appending data elements in sequence.

2. Tokens: .
   Context Sentences:
   - returnBuffer.append(FieldMetaData.Decimal.SQ_CLOSE);
   - jsonObject.getLong(Form.JSONMapping.FORM_TYPE_ID);
   - date.getTime();
   - fileReader.readLine();

   Label: Method Invocation Operator
   Semantic Tags: Dot Notation, Method Call, Property Access
   Description: The dot (.) operator is used to call methods or access properties of objects in Java.

Based on the provided tokens and context sentences below, analyze the cluster and provide your response in the following JSON format:

{{
    "Label": "Your concise label here",
    "Semantic_Tags": [
        "Tag1",
        "Tag2",
        "Tag3",
        "Tag4",
        "Tag5"
    ],
    "Description": "Your description here."
}}

Tokens: {', '.join(tokens_list)}
All Context Sentences:
{chr(10).join([f"{i + 1}. {sentence}" for i, sentence in enumerate(context_sentences)])}

Ensure your response is in valid JSON format and includes only the JSON object.
"""


    # Call the Gemini API
    response = model.generate_content(
        user_message,
        safety_settings={
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE
        }
    )

    # Parse the response text as JSON
    try:
        response_json = json.loads(response.text)
    except json.JSONDecodeError:
        print(f"Failed to parse JSON for cluster {cluster_id}. Response: {response.text}")
        response_json = {}

    label = response_json.get('Label', '')
    semantic_tags = response_json.get('Semantic_Tags', [])
    description = response_json.get('Description', '')

    # Add the response to the final output list
    final_output.append({
        "c"+cluster_id: {
            "Unique tokens": tokens_list,
            "Label": label,
            "Semantic Tags": semantic_tags,
            "Description": description
        }
    })

# Save the final output to a JSON file
output_file = r'/content/NeuroX/examples/clustering_output/Gemini_Labels.json' #change to the path where you want to save the json file
with open(output_file, 'w') as outfile:
    json.dump(final_output, outfile, indent=4)

print(f"JSON file saved to {output_file}")

# """### Cluster Statistics Script
#
# This script processes a CSV file containing word clusters and computes various statistics about the clusters, such as average cluster size, largest/smallest clusters, and variance in cluster sizes. You also have the option to save the processed data and the computed statistics.
#
#
#
# #### To compute and print the statistics without saving:
#
# ```bash
# python cluster_statistics.py inputfilename
# ```

#### To compute statistics and save the processed DataFrame to a CSV file:

```bash
python cluster_statistics.py inputfilename --save --output_file my_output.csv
```

#### To compute statistics and save them to a JSON file:

```bash
python cluster_statistics.py inputfilename --save_stats --stats_file my_stats.json
```

#### To compute and save both the DataFrame and statistics:

```bash
python cluster_statistics.py inputfilename --save --output_file my_output.csv --save_stats --stats_file my_stats.json
```

## Hierarchical Folder Structure

### Output Folder Structure

**Absolute Path to the Output Folder:**

```
/path/to/Output
```

### Dataset Structure Example for `Dataset_name` (e.g., `java.in`)

```
Output
  └── Dataset_name (e.g., java, cuda)
      └── Model_name (e.g., deepseek, microsoft/codebert-base)
          ├── Activations
          │   ├── layer1
          │   ├── layer2
          │   └── layerN
          ├── Clusters
          │   ├── layer1
          │   ├── layer2
          │   └── layerN
          ├── Labelling
          │   └── (will update Clusters folder)
          └── Visualization
```

### Example Structure for `java.in` Dataset, `deepseek` Model, and Layers 1 to N

```
/path/to/Output
  └── java
      └── deepseek
          ├── Activations
          │   ├── layer1
          │   ├── layer2
          │   └── layerN
          ├── Clusters
          │   ├── layer1
          │   ├── layer2
          │   └── layerN
          ├── Labelling
          │   └── (updates Clusters folder)
          └── Visualization
```

### Explanation:

- **\`Dataset_name\`**: This corresponds to datasets like \`java.in\` or \`cuda\`.
- **\`Model_name\`**: This could be \`deepseek\`, \`microsoft/codebert-base\`, etc.
- **Layers**: Folders for each layer (\`layer1\`, \`layer2\`, ... \`layerN\`) are nested under the \`Activations\` and \`Clusters\` directories.
- **\`Labelling\`**: This folder will contain updates related to the \`Clusters\` folder, similar to the database folder from your project.
- **\`Visualization\`**: This folder the web app scripts

"""



"""## Extras

#### Extract representations(features/neuron activations): All layers-

*[Ignore this part for now. it is only for reference. We will use the next part which does the same thing but separately for each layer. ]*

The following code is used to extract neuron values from the hidden layers of a language model. The extracted neuron activations are saved to a test_activations.json file.

input: test.in
output: test_activations.json
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/NeuroX/examples/
import NeuroX.neurox.data.extraction.transformers_extractor as transformers_extractor
transformers_extractor.extract_representations('bert-base-uncased',
    'test.in',
    'test_activations.json',
    aggregation="average" #last, first
)

"""#### Structure of Each Line in the JSON File:

- **linex_idx**: Sentence index
- **features**: List of tokens (with their activations)
  - **token**: The current token
  - **layers**: List of layers
    - **index**: Layer index (does not correspond to original model’s layers)
    - **values**: List of activation values for all neurons in the layer

"""

# !cut -c1-120 < /content/NeuroX/examples/test_activations.json

"""### Run agglomerative clustering pipeline with synthetic data

The below code generates required synthetic data (points and vocab files) and uses it to perform clustering. The output is a model(pkl file) and clusters-k.txt file where k is the number of clusters.

You can also provide your own files. There are some saved(also synthetically generated) files here /content/NeuroX/examples/clustering_output

"""

#pip install dill if it doesnt work
from NeuroX.conceptx.algorithms.clustering.agglomerative import AgglomerativeClusteringPipeline
# Initialize the pipeline with default parameters
pipeline = AgglomerativeClusteringPipeline(output_path='/content/NeuroX/examples/clustering_output', num_clusters=3)

#For synthetic data
points, vocab = pipeline.load_and_prepare_data(num_points=10, num_dims=3, vocab_size=10)
# Run the clustering pipeline
clustering, clusters = pipeline.run_pipeline(points, vocab)

num_points (int): Number of synthetic data points (if generating synthetic data).
num_dims (int): Number of dimensions for each data point (if generating synthetic data).
vocab_size (int): Size of the vocabulary (if generating synthetic data).
'''

# Plot the dendrogram for visualization
linkage_matrix = pipeline.create_linkage_matrix(points)
pipeline.plot_dendrogram(linkage_matrix)



